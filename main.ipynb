{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T16:24:28.667968Z",
     "start_time": "2024-03-09T16:24:28.665249Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T16:24:31.504706Z",
     "start_time": "2024-03-09T16:24:28.687779Z"
    }
   },
   "outputs": [],
   "source": [
    "df_accounts = pd.read_csv('data/accounts.csv')\n",
    "df_alliance_ff = pd.read_csv('data/alliance_ff.csv')\n",
    "df_alliance_membership = pd.read_csv('data/alliance_membership.csv')\n",
    "df_chat_messages_1 = pd.read_csv('data/chat_messages_1.csv', dtype='unicode')\n",
    "df_chat_messages_2 = pd.read_csv('data/chat_messages_2.csv', dtype='unicode')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clean dataset chat_messages_1   "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Drop unnecessary columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T16:24:32.362749Z",
     "start_time": "2024-03-09T16:24:31.506450Z"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['risk', 'filter_detected_language', 'is_family_friendly', 'BULLYING', 'VIOLENCE', 'RELATIONSHIP_SEXUAL_CONTENT', 'VULGARITY', 'DRUGS_ALCOHOL', 'IN_APP', 'ALARM', 'FRAUD', 'HATE_SPEECH', 'RELIGIOUS', 'WEBSITE', 'CHILD_GROOMING', 'PUBLIC_THREAT', 'EXTREMISM', 'SUBVERSIVE', 'SENTIMENT', 'POLITICS']\n",
    "df_chat_messages_1.drop(columns_to_drop, inplace=True, axis=1)\n",
    "df_chat_messages_1.to_csv(\"modified_chat_messages_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clean dataset chat_messages_2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Drop unnecessary columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns_to_drop2 = ['risk', 'filter_detected_language', 'is_family_friendly', 'BULLYING', 'VIOLENCE', 'RELATIONSHIP_SEXUAL_CONTENT', 'VULGARITY', 'DRUGS_ALCOHOL', 'IN_APP', 'ALARM', 'FRAUD', 'HATE_SPEECH', 'RELIGIOUS', 'WEBSITE', 'CHILD_GROOMING', 'PUBLIC_THREAT', 'EXTREMISM', 'SUBVERSIVE', 'SENTIMENT', 'POLITICS']\n",
    "df_chat_messages_2.drop(columns_to_drop2, inplace=True, axis=1)\n",
    "df_chat_messages_2.to_csv(\"modified_chat_messages_2.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Concat two csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_modified1 = pd.read_csv(\"modified_data/modified_chat_messages_1.csv\", dtype='unicode')\n",
    "df_modified2 = pd.read_csv(\"modified_data/modified_chat_messages_2.csv\", dtype='unicode')\n",
    "\n",
    "combined_df = pd.concat([df_modified1, df_modified2], ignore_index=True)\n",
    "combined_df.to_csv(\"merged_message_data.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add offensive column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"merged_message_data.csv\", dtype='unicode')\n",
    "merged_df['GENERAL_RISK'] = pd.to_numeric(merged_df['GENERAL_RISK'], errors='coerce')\n",
    "merged_df['offensive'] = (merged_df['GENERAL_RISK'] >= 5).astype(int)\n",
    "merged_df.to_csv(\"offensive_added.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Concat two csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_modified1 = pd.read_csv(\"modified_data/modified_chat_messages_1.csv\", dtype='unicode')\n",
    "df_modified2 = pd.read_csv(\"modified_data/modified_chat_messages_2.csv\", dtype='unicode')\n",
    "\n",
    "combined_df = pd.concat([df_modified1, df_modified2], ignore_index=True)\n",
    "combined_df.to_csv(\"merged_message_data.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-offensive\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import torch.optim as optim  # Import optimizer class\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(\"offensive_added.csv\", dtype='unicode')\n",
    "\n",
    "# Prepare text data for the model\n",
    "cleaned_data = data[data[\"raw_message\"].notna()]\n",
    "filtered_data = cleaned_data[cleaned_data[\"raw_message\"].str.isalpha()]\n",
    "text = [item for sublist in filtered_data[\"raw_message\"].tolist() for item in sublist]\n",
    "encoded_data = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Freeze the pre-trained model layers (optional)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze pre-trained weights\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.AdamW(model.classifier.parameters(), lr=2e-5)  # Adjust learning rate as needed\n",
    "loss_fn = nn.CrossEntropyLoss()  # Cross-entropy loss for binary classification\n",
    "\n",
    "# Define batch_size before the training loop\n",
    "batch_size = 32  # Adjust this value as needed\n",
    "\n",
    "# Training loop\n",
    "epochs = 3  # Adjust number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle data for each epoch (optional)\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    cleaned_data = data[data[\"raw_message\"].notna()]\n",
    "    filtered_data = cleaned_data[cleaned_data[\"raw_message\"].str.isalpha()]\n",
    "    text = [item for sublist in filtered_data[\"raw_message\"].tolist() for item in sublist]\n",
    "    encoded_data = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Training loop per batch\n",
    "    for batch_idx in range(len(encoded_data) // batch_size):\n",
    "        input_ids = encoded_data['input_ids'][batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "        attention_mask = encoded_data['attention_mask'][batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "        labels = torch.tensor(data[\"offensive\"][batch_idx*batch_size:(batch_idx+1)*batch_size])  # Assuming \"label\" is your binary classification column\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Backward pass and parameter update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training progress (optional)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch: {epoch+1}/{epochs}, Batch: {batch_idx+1}/{len(encoded_data) // batch_size}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(encoded_data['input_ids'], data[\"offensive\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Training loop with validation\n",
    "epochs = 3  # Adjust number of epochs\n",
    "best_val_f1 = 0.0  # Initialize best validation F1-score\n",
    "early_stopping_count = 0  # Counter for early stopping\n",
    "for epoch in range(epochs):\n",
    "    # ... your existing training loop per batch\n",
    "\n",
    "    # Evaluate on validation set after each epoch\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val, attention_mask=X_val['attention_mask'])\n",
    "        val_logits = val_outputs.logits\n",
    "        val_loss = loss_fn(val_logits, y_val)\n",
    "        val_predictions = torch.argmax(val_logits, dim=1)\n",
    "\n",
    "        # Calculate F1-score (or other metrics)\n",
    "        from sklearn.metrics import f1_score\n",
    "        val_f1 = f1_score(y_val, val_predictions)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}/{epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Early stopping (optional)\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            early_stopping_count = 0  # Reset counter\n",
    "            # Optionally, save the best model here\n",
    "        else:\n",
    "            early_stopping_count += 1\n",
    "            if early_stopping_count >= 5:  # Adjust patience as needed\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# After training, use classification report\n",
    "y_pred = torch.argmax(model(X_test['input_ids'], attention_mask=X_test['attention_mask']).logits, dim=1)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the fine-tuned model (optional)\n",
    "model.save_pretrained(\"your_fine_tuned_model\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T15:32:55.096462Z",
     "start_time": "2024-03-11T15:32:30.458041Z"
    }
   },
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
